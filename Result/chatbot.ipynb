{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIRmPKncYeeX",
        "outputId": "c577b880-c0b6-4370-cb1f-2c22fca23f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wc8vVM9KXnV0",
        "outputId": "9265fe90-198b-4188-a0f5-df6a338aaa84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(30000, 768, padding_idx=3)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_save_path = '/content/drive/MyDrive/KUBIG/여름CONTEST/24S NLP Basic Team 4/model/translation_model_st_jj'\n",
        "\n",
        "tokenizer_save_path = '/content/drive/MyDrive/KUBIG/여름CONTEST/24S NLP Basic Team 4/model/translation_tokenizer_st_jj'\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
        "\n",
        "model_int_jj = BartForConditionalGeneration.from_pretrained(model_save_path)\n",
        "tokenizer_int_jj = PreTrainedTokenizerFast.from_pretrained(tokenizer_save_path)\n",
        "model_int_jj.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYZlsGNmY51U",
        "outputId": "6bff2c2d-5d73-435d-c730-c2b10ce7a9ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "저리로 영 너 나가서라 하면 나가고\n"
          ]
        }
      ],
      "source": [
        "# 입력 문장\n",
        "input_sentence = \"저리로 이렇게 너 나가있어라 하면 나가고\"\n",
        "\n",
        "# 토큰화\n",
        "input_ids = tokenizer_int.encode(input_sentence, return_tensors = \"pt\").to(device)\n",
        "\n",
        "# 모델 예측\n",
        "output_ids = model_int.generate(input_ids, max_length = 50, num_beams = 5, early_stopping = True)\n",
        "\n",
        "# 결과 해독\n",
        "output_sentence = tokenizer_int.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Zvww8gpZMAj",
        "outputId": "f548e81a-d56a-4ee5-955d-7f6945c907f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전화 오는 거 닮은디\n"
          ]
        }
      ],
      "source": [
        "# 입력 문장\n",
        "input_sentence = \"전화 오는 거 같은데\"\n",
        "\n",
        "# 토큰화\n",
        "input_ids = tokenizer.encode(input_sentence, return_tensors = \"pt\").to(device)\n",
        "\n",
        "# 모델 예측\n",
        "output_ids = model.generate(input_ids, max_length = 50, num_beams = 5, early_stopping = True)\n",
        "\n",
        "# 결과 해독\n",
        "output_sentence = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJP8VNqNft7u"
      },
      "source": [
        "### **KoGPT2 (All)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3k2odouZVW_",
        "outputId": "b53de0ce-f52e-41d1-ddc7-5d1fd85f1c90"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(51200, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_save_path = '/content/drive/MyDrive/KUBIG/여름CONTEST/24S NLP Basic Team 4/code/채원/KoGPT2(with AllData)_1st/model'\n",
        "\n",
        "tokenizer_save_path = '/content/drive/MyDrive/KUBIG/여름CONTEST/24S NLP Basic Team 4/code/채원/KoGPT2(with AllData)_1st/tokenizer'\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "\n",
        "model_chatbot = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
        "tokenizer_chatbot = PreTrainedTokenizerFast.from_pretrained(tokenizer_save_path)\n",
        "model_chatbot.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpqYeWYXaQMW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "Q_TKN = \"<usr>\"\n",
        "A_TKN = \"<sys>\"\n",
        "BOS = '</s>'\n",
        "EOS = '</s>'\n",
        "MASK = '<unused0>'\n",
        "SENT = '<unused1>'\n",
        "PAD = '<pad>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "V8NYvi34Z_1N",
        "outputId": "cce98311-3cd6-471f-f674-163da76ee5be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user > 요즘 날씨 어때?\n",
            "Chatbot > 요즘 선선하니 딱 좋아너무 더워서 싫더라\n",
            "user > 둘이 사귀면 잘 어울리겠다 으휴\n",
            "Chatbot > 아주 한 쌍의 바퀴벌레가 탄생할뻔 했지 알바하면서 잘생긴 손님은 본 기억이 거의 없어너는 인\n",
            "user > 난 커널스 카라멜이 젤 맛있어 아삭바삭해\n",
            "Chatbot > 난 지금 배고파서 뭐든 입에 넣으면 맛있을 거 닮아\n",
            "user > 그래서 넌 언제 해 \n",
            "Chatbot > 나 무사 일년 전에 하지 않안\n",
            "user > 밥 안 먹었더니 배고파\n",
            "Chatbot > 야 너 너무 착하다난 절대 못 먹어\n",
            "user > 저녁 뭐 먹을까?\n",
            "Chatbot > 이제 커피는 안마셔서 추천해줘\n",
            "user > 밥 뭐 먹을까?\n",
            "Chatbot > 이제 막걸리에 환장하지\n",
            "user > 오늘 고기 먹으러 가자 했다가 실패했어\n",
            "Chatbot > 그럼 너가 맛있는 걸 사오는 건 어때\n",
            "user >  그거 지금도 가능해 돈만 있으면 \n",
            "Chatbot > 그래 너 돈 많다고 이것저것 다 배달 시키구 그러는 거 아니지\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-f5e9120b00e3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user > \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    while 1:\n",
        "        q = input(\"user > \").strip()\n",
        "        if q == \"quit\":\n",
        "            break\n",
        "        a = \"\"\n",
        "        while 1:\n",
        "            input_ids = torch.LongTensor(tokenizer_chatbot.encode(Q_TKN + q + SENT + A_TKN + a)).unsqueeze(dim=0)\n",
        "            model_chatbot = model_chatbot.to('cpu')\n",
        "            pred = model_chatbot(input_ids)\n",
        "            pred = pred.logits\n",
        "            gen = tokenizer_chatbot.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1]\n",
        "            if gen == EOS:\n",
        "                break\n",
        "            a += gen.replace(\"▁\", \" \")\n",
        "        input_ids = tokenizer_int_jj.encode(a, return_tensors = \"pt\").to(device)\n",
        "        # 모델 예측\n",
        "        output_ids = model_int_jj.generate(input_ids, max_length = 50, num_beams = 5, early_stopping = True)\n",
        "        # 결과 해독\n",
        "        output_sentence = tokenizer_int_jj.decode(output_ids[0], skip_special_tokens=True)\n",
        "        print(\"Chatbot > {}\".format(output_sentence.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thi2jrg0ZjYl",
        "outputId": "84220acd-8378-4124-e4c0-63b89c66870b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "무슨 그런 게 다 이서? 신기하네 전화해봐\n"
          ]
        }
      ],
      "source": [
        "# 입력 문장\n",
        "input_sentence = \" 무슨 그런 게 다 있어? 신기하네 전화해봐\"\n",
        "\n",
        "# 토큰화\n",
        "input_ids = tokenizer_int.encode(input_sentence, return_tensors = \"pt\").to(device)\n",
        "\n",
        "# 모델 예측\n",
        "output_ids = model_int.generate(input_ids, max_length = 50, num_beams = 5, early_stopping = True)\n",
        "\n",
        "# 결과 해독\n",
        "output_sentence = tokenizer_int.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pt02dXUeU4V",
        "outputId": "14403d2d-d7ad-47b3-d33c-9dc69f3462c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(30000, 768, padding_idx=3)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartEncoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x BartDecoderLayer(\n",
              "          (self_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartSdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_save_path = '/content/drive/MyDrive/KUBIG/여름CONTEST/24S NLP Basic Team 4/model/translation_model_st_cc'\n",
        "\n",
        "tokenizer_save_path = '/content/drive/MyDrive/KUBIG/여름CONTEST/24S NLP Basic Team 4/model/translation_tokenizer_st_cc'\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import BartForConditionalGeneration, PreTrainedTokenizerFast\n",
        "\n",
        "model_int_cc = BartForConditionalGeneration.from_pretrained(model_save_path)\n",
        "tokenizer_int_cc = PreTrainedTokenizerFast.from_pretrained(tokenizer_save_path)\n",
        "model_int_cc.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPxOKGWGe6NP",
        "outputId": "7323eda4-1e82-47cb-d6d1-2e95d65fbc2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "너무 좀 드럽고 그래 가주고\n"
          ]
        }
      ],
      "source": [
        "# 입력 문장\n",
        "input_sentence = \"너무 좀 더럽고 그래 가지고\"\n",
        "\n",
        "# 토큰화\n",
        "input_ids = tokenizer_int_cc.encode(input_sentence, return_tensors = \"pt\").to(device)\n",
        "\n",
        "# 모델 예측\n",
        "output_ids = model_int_cc.generate(input_ids, max_length = 50, num_beams = 5, early_stopping = True)\n",
        "\n",
        "# 결과 해독\n",
        "output_sentence = tokenizer_int_cc.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "H44l8olyfDwn",
        "outputId": "239309c7-1f97-4719-9556-6b3f23d77fbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user >  아주머니 한 분 계시더라구\n",
            "Chatbot > 뭐만 하면 다 돈이라고 그래 가주고 너무 싫더라\n",
            "user > 그니까 그것도 던킨이 그럴 줄 몰랐어\n",
            "Chatbot > 진짜    던킨이 그럴 줄 알았어\n",
            "user > 오잉 ... 에콰도르 야들은 또 와 이런데\n",
            "Chatbot > 왜 무슨 일이 있는 겨.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-4adfdb6f5405>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user > \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    while 1:\n",
        "        q = input(\"user > \").strip()\n",
        "        if q == \"quit\":\n",
        "            break\n",
        "        a = \"\"\n",
        "        while 1:\n",
        "            input_ids = torch.LongTensor(tokenizer_chatbot.encode(Q_TKN + q + SENT + A_TKN + a)).unsqueeze(dim=0)\n",
        "            model_chatbot = model_chatbot.to('cpu')\n",
        "            pred = model_chatbot(input_ids)\n",
        "            pred = pred.logits\n",
        "            gen = tokenizer_chatbot.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1]\n",
        "            if gen == EOS:\n",
        "                break\n",
        "            a += gen.replace(\"▁\", \" \")\n",
        "        input_ids = tokenizer_int_cc.encode(a, return_tensors = \"pt\").to(device)\n",
        "        # 모델 예측\n",
        "        output_ids = model_int_cc.generate(input_ids, max_length = 50, num_beams = 5, early_stopping = True)\n",
        "        # 결과 해독\n",
        "        output_sentence = tokenizer_int_cc.decode(output_ids[0], skip_special_tokens=True)\n",
        "        print(\"Chatbot > {}\".format(output_sentence.strip()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBk8TMlgfxyO"
      },
      "source": [
        "### **KoGPT2 (Chatbot Data)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DPmDqC-fxk_",
        "outputId": "423c7648-d985-4b1e-cd94-c2cea008890b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(51200, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_save_path = '/content/drive/MyDrive/KUBIG/여름CONTEST/24S NLP Basic Team 4/code/채원/KoGPT2 (with ChatbotData)/model_chatbotdata'\n",
        "\n",
        "tokenizer_save_path = '/content/drive/MyDrive/KUBIG/여름CONTEST/24S NLP Basic Team 4/code/채원/KoGPT2 (with ChatbotData)/tokenizer_chatbotdata'\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "\n",
        "model_chatbot2 = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
        "tokenizer_chatbot2 = PreTrainedTokenizerFast.from_pretrained(tokenizer_save_path)\n",
        "model_chatbot2.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3STCIDk1hJlN"
      },
      "source": [
        "#### 충청도"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "aFNnetfBfKZM",
        "outputId": "7fbe5d06-d5db-4861-f8f1-96a0ff51d311"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user > 밥 먹었어?\n",
            "Chatbot > 밥심으로 사는 거죠\n",
            "user > 뭐 먹었는데?\n",
            "Chatbot > 기분나쁘겠어요\n",
            "user > 기분 안나쁜데\n",
            "Chatbot > 정색 한번 해주세요\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2c45fcc79f95>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user > \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    while 1:\n",
        "        q = input(\"user > \").strip()\n",
        "        if q == \"quit\":\n",
        "            break\n",
        "        a = \"\"\n",
        "        while 1:\n",
        "            input_ids = torch.LongTensor(tokenizer_chatbot2.encode(Q_TKN + q + SENT + A_TKN + a)).unsqueeze(dim=0)\n",
        "            model_chatbot2 = model_chatbot2.to('cpu')\n",
        "            pred = model_chatbot2(input_ids)\n",
        "            pred = pred.logits\n",
        "            gen = tokenizer_chatbot2.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1]\n",
        "            if gen == EOS:\n",
        "                break\n",
        "            a += gen.replace(\"▁\", \" \")\n",
        "        input_ids = tokenizer_int_cc.encode(a, return_tensors = \"pt\").to(device)\n",
        "        # 모델 예측\n",
        "        output_ids = model_int_cc.generate(input_ids, max_length = 50, num_beams = 5, early_stopping = True)\n",
        "        # 결과 해독\n",
        "        output_sentence = tokenizer_int_cc.decode(output_ids[0], skip_special_tokens=True)\n",
        "        print(\"Chatbot > {}\".format(output_sentence.strip()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHb_vIpGhLaZ"
      },
      "source": [
        "#### 제주도"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "fXu8guVVgXtX",
        "outputId": "ebe257fa-8c94-4d7c-c27b-dd81278f24eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user > 왜 싸웠는지도 기억 안나\n",
            "Chatbot > 그 이유를 찾는 과정이 되겠네요\n",
            "user > 왜 싸웠는지도 기억 안나\n",
            "Chatbot > 그 이유를 찾는 과정이 되겠네요\n",
            "user > 갑자기 급우울\n",
            "Chatbot > 낮잠을 잠깐 자도 괜찮아요\n",
            "user > 연락 없더니 갑자기 왜 만나자고 하지?\n",
            "Chatbot > 만나지 마세요\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-80ef6cdb649b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user > \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    while 1:\n",
        "        q = input(\"user > \").strip()\n",
        "        if q == \"quit\":\n",
        "            break\n",
        "        a = \"\"\n",
        "        while 1:\n",
        "            input_ids = torch.LongTensor(tokenizer_chatbot2.encode(Q_TKN + q + SENT + A_TKN + a)).unsqueeze(dim=0)\n",
        "            model_chatbot2 = model_chatbot2.to('cpu')\n",
        "            pred = model_chatbot2(input_ids)\n",
        "            pred = pred.logits\n",
        "            gen = tokenizer_chatbot2.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1]\n",
        "            if gen == EOS:\n",
        "                break\n",
        "            a += gen.replace(\"▁\", \" \")\n",
        "        input_ids = tokenizer_int_jj.encode(a, return_tensors = \"pt\").to(device)\n",
        "        # 모델 예측\n",
        "        output_ids = model_int_jj.generate(input_ids, max_length = 50, num_beams = 5, early_stopping = True)\n",
        "        # 결과 해독\n",
        "        output_sentence = tokenizer_int_jj.decode(output_ids[0], skip_special_tokens=True)\n",
        "        print(\"Chatbot > {}\".format(output_sentence.strip()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhM6Ae1qlxWl"
      },
      "source": [
        "### **KoGPT2 (final)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_wQeWGulxWl",
        "outputId": "b5a8f8d0-f4c4-4c3d-a3eb-fdd45350f494"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(51200, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_save_path = '/content/drive/MyDrive/KUBIG/여름CONTEST/24S NLP Basic Team 4/model_final'\n",
        "\n",
        "tokenizer_save_path = '/content/drive/MyDrive/KUBIG/여름CONTEST/24S NLP Basic Team 4/tokenizer_final'\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast\n",
        "\n",
        "model_chatbot_3 = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
        "tokenizer_chatbot_3 = PreTrainedTokenizerFast.from_pretrained(tokenizer_save_path)\n",
        "model_chatbot_3.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pL-B84bZlxWm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
        "\n",
        "Q_TKN = \"<usr>\"\n",
        "A_TKN = \"<sys>\"\n",
        "BOS = '</s>'\n",
        "EOS = '</s>'\n",
        "MASK = '<unused0>'\n",
        "SENT = '<unused1>'\n",
        "PAD = '<pad>'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-NVbhhDlxWm",
        "outputId": "d5b58b84-eb8e-44ff-9804-f199baff4732"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user > 밥 먹었니?\n",
            "Chatbot > 응응 오늘 밥먹언\n",
            "user > 뭐 먹었어?\n",
            "Chatbot > 나 햄버거 먹언\n",
            "user > 나는 뭐 먹었게?\n",
            "Chatbot > 나 햄버거\n",
            "user > 난 커널스 카라멜이 젤 맛있어 아삭바삭해\n",
            "Chatbot > 난 지금 배고파서 그냥 그랬어\n",
            "user > 지금 비 오는 것 같아?\n",
            "Chatbot > 지금 비 오는 거 아니야\n",
            "user > 그래서 넌 언제 해?\n",
            "Chatbot > 난 요즘 하는 거 없이 바로바로 하고 인\n",
            "user > 오늘 세상에 무슨 일 있니?\n",
            "Chatbot > 무슨 일 이서\n",
            "user > 헐 그거 봤어?\n",
            "Chatbot > 그거 시즌투도 있어\n",
            "user > 디즈니 좋아하시나요?\n",
            "Chatbot > 디즈니 좋아하는 거 같애\n",
            "user > 따뜻한 차 좋아해?\n",
            "Chatbot > 나는 차가운 차가 더 좋아\n",
            "user > 따뜻한 건?\n",
            "Chatbot > 차가운 건 아닌데\n",
            "user > quit\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    while 1:\n",
        "        q = input(\"user > \").strip()\n",
        "        if q == \"quit\":\n",
        "            break\n",
        "        a = \"\"\n",
        "        while 1:\n",
        "            input_ids = torch.LongTensor(tokenizer_chatbot.encode(Q_TKN + q + SENT + A_TKN + a)).unsqueeze(dim=0)\n",
        "            model_chatbot_3 = model_chatbot_3.to('cpu')\n",
        "            pred = model_chatbot_3(input_ids)\n",
        "            pred = pred.logits\n",
        "            gen = tokenizer_chatbot_3.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1]\n",
        "            if gen == EOS:\n",
        "                break\n",
        "            a += gen.replace(\"▁\", \" \")\n",
        "        input_ids = tokenizer_int_jj.encode(a, return_tensors = \"pt\").to(device)\n",
        "        # 모델 예측\n",
        "        output_ids = model_int_jj.generate(input_ids, max_length = 50, num_beams = 5, early_stopping = True)\n",
        "        # 결과 해독\n",
        "        output_sentence = tokenizer_int_jj.decode(output_ids[0], skip_special_tokens=True)\n",
        "        print(\"Chatbot > {}\".format(output_sentence.strip()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAuZISLfnqBw",
        "outputId": "7313fb84-05dd-48be-db64-a5c0515fec9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user > 밥 먹었니?\n",
            "Chatbot > 응응 오늘 밥먹었어\n",
            "user > 나 며칠 전에 독립하고 싶어서 집 알아보고 왔음\n",
            "Chatbot > 독립하면 집도 엄청 비싸지 않아\n",
            "user >  진짜 독립을 하려는 건 아니고\n",
            "Chatbot > 근데 왜 부동산까지 가서 알아보는 겨\n",
            "user > 알바하고 많이 힘들었구나\n",
            "Chatbot > 응   힘들었지만 열심히 했지\n",
            "user > 나는 지금 학원 선택 때문에 고민이 많아\n",
            "Chatbot > 왜  어떤 과목을 선택하는데\n",
            "user > 너는 수상 스키 배워보고 싶어?\n",
            "Chatbot > 아니 난 수상스키 배워보고 싶어\n",
            "user > 그래 그러면 그냥 이번 여름에 너랑 수상 스키나 배워야겠다\n",
            "Chatbot > 그래 같이 배워보자\n",
            "user > 여수 밤바다 보면서 술 마시는 곳이래\n",
            "Chatbot > 거기 진짜 좋겠다 난 여수 밤바다 보면서 술 한 잔도 못 마심\n",
            "user > 내가 게임 너무 달달 연습했나 봐\n",
            "Chatbot > 아니야   나는 연습을 많이 했었거든\n",
            "user > 오늘 이케아 가서 소품이랑 장식들 사서 놓으려고\n",
            "Chatbot > 이케아에서 어떤 거 샀어\n",
            "user > 곧 여름이니까 스타일도 시원하게 바꾸려고\n",
            "Chatbot > 시원하게 남자들 많이 하는 거 있잖아\n",
            "user > quit\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    while 1:\n",
        "        q = input(\"user > \").strip()\n",
        "        if q == \"quit\":\n",
        "            break\n",
        "        a = \"\"\n",
        "        while 1:\n",
        "            input_ids = torch.LongTensor(tokenizer_chatbot.encode(Q_TKN + q + SENT + A_TKN + a)).unsqueeze(dim=0)\n",
        "            model_chatbot_3 = model_chatbot_3.to('cpu')\n",
        "            pred = model_chatbot_3(input_ids)\n",
        "            pred = pred.logits\n",
        "            gen = tokenizer_chatbot_3.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().numpy().tolist())[-1]\n",
        "            if gen == EOS:\n",
        "                break\n",
        "            a += gen.replace(\"▁\", \" \")\n",
        "        input_ids = tokenizer_int_cc.encode(a, return_tensors = \"pt\").to(device)\n",
        "        # 모델 예측\n",
        "        output_ids = model_int_cc.generate(input_ids, max_length = 50, num_beams = 5, early_stopping = True)\n",
        "        # 결과 해독\n",
        "        output_sentence = tokenizer_int_cc.decode(output_ids[0], skip_special_tokens=True)\n",
        "        print(\"Chatbot > {}\".format(output_sentence.strip()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit ('3.10.6')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "a6bbcf36dc5df263ca68502d6771d4ac689cc117f1ed616cc5072a0f313fc5b7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
